/*****************************************************************************\
* (c) Copyright 2000-2018 CERN for the benefit of the LHCb Collaboration      *
*                                                                             *
* This software is distributed under the terms of the GNU General Public      *
* Licence version 3 (GPL Version 3), copied verbatim in the file "COPYING".   *
*                                                                             *
* In applying this licence, CERN does not waive the privileges and immunities *
* granted to it by virtue of its status as an Intergovernmental Organization  *
* or submit itself to any jurisdiction.                                       *
\*****************************************************************************/
// Class: ReadMLP
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/02       [394754]
Creator        : decian
Date           : Thu May 19 14:37:12 2016
Host           : Linux lcgapp-slc6-physical1.cern.ch 2.6.32-573.8.1.el6.x86_64 #1 SMP Wed Nov 11 15:27:45 CET 2015
x86_64 x86_64 x86_64 GNU/Linux Dir            : /misc/home/he/decian/PatLongLivedTrackingMVA Training events: 2000
Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "600" [Number of training cycles]
HiddenLayers: "N+5" [Specification of hidden layer architecture]
NeuronType: "ReLU" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g.,
"D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for
the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is
assumed)"] H: "True" [Print method-specific help message] TestRate: "5" [Test for overtraining performed at each #th
epochs] UseRegulator: "False" [Use regulator to avoid over-training] # Default: RandomSeed: "1" [Random seed for initial
synapse weights (0 means unique seed for each run; default value '1')] NeuronInputType: "sum" [Neuron input function
type] VerbosityLevel: "Default" [Verbosity level] CreateMVAPdfs: "False" [Create PDFs for classifier outputs (signal and
background)] IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are
included for testing and performance evaluation)] TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm
(BFGS), or Genetic Algorithm (GA - slower and worse)] LearningRate: "2.000000e-02" [ANN learning rate parameter]
DecayRate: "1.000000e-02" [Decay rate for learning parameter]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output
file!)] Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch] SamplingEpoch:
"1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than
before) are multiplied with SamplingImportance, else they are divided.] SamplingTraining: "True" [The training sample is
sampled] SamplingTesting: "False" [The testing sample is sampled] ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence
check is turned off)] ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means
automatic convergence check is turned off)] UpdateLimit: "10000" [Maximum times of regulator update] CalculateErrors:
"False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an
MVA value] WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the
desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 9
chi2                          chi2                          chi2                          chi2 'F'
[0.000526935153175,19.9912433624] seedChi2                      seedChi2                      seedChi2 seedChi2 'F'
[0.0201481822878,4.06411504745] p                             p                             p p 'F'
[1425.44555664,117527.453125] pt                            pt                            pt pt 'F'
[7.08713912964,4357.78515625] deltaP                        deltaP                        deltaP deltaP 'F'
[-0.128111407161,0.190970674157] deviation                     deviation                     deviation deviation 'F'
[0.714163601398,342.545349121] initialChi2                   initialChi2                   initialChi2 initialChi2 'F'
[0.000174650442204,39.7950744629] nHits                         nHits                         nHits nHits 'F'    [2,5]
highThresHits                 highThresHits                 highThresHits                 highThresHits 'F'    [1,5]
NSpec 0


============================================================================ */

#include <array>
#include <cmath>
#include <iostream>
#include <numeric>
#include <string>
#include <vector>

#ifndef IClassifierReaderForLLT__def
#  define IClassifierReaderForLLT__def

class IClassifierReaderForLLT {

public:
  // constructor
  IClassifierReaderForLLT() : fStatusIsClean( true ) {}
  virtual ~IClassifierReaderForLLT() {}

  // return classifier response
  virtual double GetMvaValue( const std::vector<double>& inputValues ) const = 0;

  // returns classifier status
  bool IsStatusClean() const { return fStatusIsClean; }

protected:
  bool fStatusIsClean;
};

#endif

class NNForLLT : public IClassifierReaderForLLT {

public:
  // constructor
  NNForLLT( const std::vector<std::string>& theInputVars ) {
    // the training input variables
    const char* inputVars[] = { "chi2",      "seedChi2",    "p",     "pt",           "deltaP",
                                "deviation", "initialChi2", "nHits", "highThresHits" };

    // sanity checks
    if ( theInputVars.size() <= 0 ) {
      std::cout << "Problem in class \"" << fClassName << "\": empty input vector" << std::endl;
      fStatusIsClean = false;
    }

    if ( theInputVars.size() != fNvars ) {
      std::cout << "Problem in class \"" << fClassName
                << "\": mismatch in number of input values: " << theInputVars.size() << " != " << fNvars << std::endl;
      fStatusIsClean = false;
    }

    // validate input variables
    for ( size_t ivar = 0; ivar < theInputVars.size(); ivar++ ) {
      if ( theInputVars[ivar] != inputVars[ivar] ) {
        std::cout << "Problem in class \"" << fClassName << "\": mismatch in input variable names" << std::endl
                  << " for variable [" << ivar << "]: " << theInputVars[ivar].c_str() << " != " << inputVars[ivar]
                  << std::endl;
        fStatusIsClean = false;
      }
    }
  }

  // the classifier response
  // "inputValues" is a vector of input values in the same order as the
  // variables given to the constructor
  double GetMvaValue( const std::vector<double>& inputValues ) const override;

private:
  // common member variables
  static constexpr const char* fClassName = "NNForLLT";

  constexpr static size_t fNvars = 9;
  size_t                  GetNvar() const { return fNvars; }
  char                    GetType( int ivar ) const {
    // type of input variable: 'F' or 'I'
    static constexpr char fType[fNvars] = { 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F' };
    return fType[ivar];
  }

  // input variable transformation

  void Transform( std::array<double, fNvars>& iv, int sigOrBgd ) const;

  // normalisation of input variables
  constexpr bool IsNormalised() const { return false; }

  // initialize internal variables
  double GetMvaValue__( const std::array<double, fNvars>& inputValues ) const;
};

double NNForLLT::GetMvaValue__( const std::array<double, fNvars>& inputValues ) const {
  auto ActivationFnc = []( double x ) {
    // rectified linear unit
    return x > 0 ? x : 0;
  };
  auto OutputActivationFnc = []( double x ) {
    // sigmoid
    return 1.0 / ( 1.0 + exp( -x ) );
  };
  // build network structure
  constexpr int                      fLayers    = 3;
  constexpr std::array<int, fLayers> fLayerSize = { 10, 15, 1 };
  // weight matrix from layer 0 to 1
  static constexpr double fWeightMatrix0to1[fLayerSize[1]][fLayerSize[0]] = {
      { -0.159988061216097, -0.0288443527641527, -4.14974329044045, 3.81105644239148, 1.22170243017733,
        0.958687397983864, 1.96383198084087, -0.390165101096577, -0.96260098791518, 3.1693091792906 },
      { 0.991198675856357, 1.43711861414492, 0.848030606613098, 2.40790463923064, -1.82736032237584, -1.86596393924827,
        0.715103221827321, 0.0473705763898225, -1.18568489901349, 0.999278392705942 },
      { 1.23398891345619, -0.29412009203639, 0.216258450266045, -1.119236438548, 1.74495336284469, 0.883613416326042,
        1.27121676033308, -2.25593259646822, -1.21547716362445, -1.32105124904016 },
      { 1.72722031910379, 1.60953159884603, 2.45574543887207, -0.33325000983855, -0.214050078690866, -0.088853589329277,
        -1.6135758298526, -1.34160476246501, 2.55112093270656, 0.706777214333567 },
      { -0.930744996129737, 0.442866020718777, -0.168849720636411, 1.36961082760777, -2.96406717370372,
        0.189704580988342, -1.17698814313625, 0.31640889497951, -1.05844432017337, -1.19336982392844 },
      { -1.43856692000168, 0.103462200680609, 0.0054408330523039, 1.36779349315227, 0.35729013243452, 0.977421944976319,
        -1.79840299595851, 2.85142613379838, -1.81743842404449, -0.0680045449604156 },
      { -1.38247784143833, -0.720255354245469, 0.377841634588054, 2.56068293292282, 0.27677946701197,
        -0.214505561240179, 0.654747453366814, -0.0157733988064166, 2.14019497585393, 2.04162106960511 },
      { 1.91776900836508, 1.12069669855829, -0.912502872739713, 1.60411688000255, 0.169943856861226, -2.43222613326565,
        0.269100219067868, 1.66498890906353, -0.736666461782157, -1.34155103245281 },
      { -1.03643332796002, 1.02303203651558, -0.812776485958405, 1.3785380559435, 4.68426940750364, 1.12423127838535,
        -0.209021890938452, 1.02860558305364, -1.42015535880605, 0.515866793005734 },
      { -0.461179079439787, -0.434214116160295, 1.79426102036118, 1.3208881167651, 0.4301946994156, 1.66861596269273,
        -2.34180310027017, -0.101416939440672, 1.09597892727315, 1.8044372338864 },
      { 0.19963900991783, -0.00592440082074486, 1.06116396151975, -4.26724358021828, 2.68708191957747, 1.79001374505289,
        -0.00685616925372599, -2.39914521401491, 0.406323686243346, -0.442924596318703 },
      { -0.41367709543556, -0.22618842497468, 1.11355694197118, 1.31841345876455, -1.20979642588645, 0.642542898654938,
        1.61348068527877, 0.337556495331228, 1.79975253436714, -1.88079455960542 },
      { -1.84453275645836, 1.79891870711404, 0.816899594555126, -1.06415371249484, 0.653923096742488, 1.55015264285264,
        0.299478873567986, -1.67708852068458, 0.823530828935443, -0.345541969065072 },
      { -0.0952806431493381, 0.102267238010965, 2.93058479179663, -7.52094716423401, -0.653064377767366,
        0.347018999360103, 0.339076424183956, 0.331157163981209, 0.0911233090662068, -3.63251641981164 } };
  // weight matrix from layer 1 to 2
  static constexpr double fWeightMatrix1to2[fLayerSize[2]][fLayerSize[1]] = {
      { 1.05108741619712, -2.33537137207328, -1.88646245683149, 1.31693484828441, -1.55900720975402, 0.80017580453241,
        0.726715711261941, -1.55149718983795, -3.18221526311605, 1.04995209715017, -1.86811068513899, 0.609195512719452,
        1.35413591925158, -3.80054783510394, -0.802693882471588 } };

  std::array<double, fLayerSize[1] - 1 + fLayerSize[2]> fWeights = { 0 };

  // layer 0 to 1
  for ( int o = 0; o < fLayerSize[1] - 1; o++ ) {
    fWeights[o] = ActivationFnc( std::inner_product( begin( inputValues ), end( inputValues ), fWeightMatrix0to1[o],
                                                     fWeightMatrix0to1[o][fLayerSize[0] - 1] ) );
  }
  // layer 1 to 2
  for ( int o = 0; o < fLayerSize[2]; o++ ) {
    fWeights[fLayerSize[1] - 1 + o] =
        OutputActivationFnc( std::inner_product( begin( fWeights ), begin( fWeights ) + fLayerSize[1] - 1,
                                                 fWeightMatrix1to2[o], fWeightMatrix1to2[o][fLayerSize[1] - 1] ) );
  }
  return fWeights[fLayerSize[1] - 1 + 0];
}

double NNForLLT::GetMvaValue( const std::vector<double>& inputValues ) const {
  if ( inputValues.size() != fNvars ) {
    std::cout << "Input vector needs to be of size " << fNvars << std::endl;
    return 0;
  }
  // initialize min and max vectors (for normalisation)
  static constexpr double fVmin[fNvars] = { -1, -1, -1, -1, -1, -1, -1, -1, -1 };
  static constexpr double fVmax[fNvars] = { 0.99999988079071, 1, 1, 1, 1, 1, 1, 1, 1 };
  // classifier response value
  double retval = 0;

  // classifier response, sanity check first
  if ( !IsStatusClean() ) {
    std::cout << "Problem in class \"" << fClassName << "\": cannot return classifier response"
              << " because status is dirty" << std::endl;
    retval = 0;
  } else {
    if ( IsNormalised() ) {
      // normalise variables
      auto NormVariable = []( double x, double xmin, double xmax ) {
        // normalise to output range: [-1, 1]
        return 2 * ( x - xmin ) / ( xmax - xmin ) - 1.0;
      };
      std::array<double, fNvars> iV;
      int                        ivar = 0;
      for ( auto varIt = inputValues.begin(); varIt != inputValues.end(); varIt++, ivar++ ) {
        iV[ivar] = NormVariable( *varIt, fVmin[ivar], fVmax[ivar] );
      }
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    } else {
      std::array<double, fNvars> iV;
      std::copy( begin( inputValues ), end( inputValues ), begin( iV ) );
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    }
  }

  return retval;
}

//_______________________________________________________________________
inline void NNForLLT::Transform( std::array<double, fNvars>& iv, int cls ) const {
  // Normalization transformation, initialisation
  constexpr double fMin_1[3][fNvars] = { { 0.00772825069726, 0.0201481822878, 1437.72265625, 27.5935173035,
                                           -0.0820798203349, 0.714163601398, 0.000174650442204, 2, 2 },
                                         { 0.000526935153175, 0.0209721103311, 1425.44555664, 7.08713912964,
                                           -0.128111407161, 1.3154027462, 0.000238049440668, 2, 1 },
                                         { 0.000526935153175, 0.0201481822878, 1425.44555664, 7.08713912964,
                                           -0.128111407161, 0.714163601398, 0.000174650442204, 2, 1 } };
  constexpr double fMax_1[3][fNvars] = { { 19.9912433624, 3.85413146019, 117527.453125, 4159.05957031, 0.0838460996747,
                                           342.545349121, 39.7950744629, 5, 5 },
                                         { 19.8668727875, 4.06411504745, 87145.0703125, 4357.78515625, 0.190970674157,
                                           290.415283203, 39.7268753052, 5, 5 },
                                         { 19.9912433624, 4.06411504745, 117527.453125, 4357.78515625, 0.190970674157,
                                           342.545349121, 39.7950744629, 5, 5 } };
  // Normalization transformation
  if ( cls < 0 || cls > 2 ) {
    if ( 2 > 1 )
      cls = 2;
    else
      cls = 2;
  }
  // get indices of used variables

  // define the indices of the variables which are transformed by this transformation
  constexpr std::array<int, fNvars> indicesGet = { 0, 1, 2, 3, 4, 5, 6, 7, 8 };
  constexpr std::array<int, fNvars> indicesPut = { 0, 1, 2, 3, 4, 5, 6, 7, 8 };
  std::array<double, fNvars>        dv;
  for ( unsigned ivar = 0; ivar < fNvars; ivar++ ) dv[ivar] = iv[indicesGet.at( ivar )];
  for ( unsigned ivar = 0; ivar < fNvars; ivar++ ) {
    double offset             = fMin_1[cls][ivar];
    double scale              = 1.0 / ( fMax_1[cls][ivar] - fMin_1[cls][ivar] );
    iv[indicesPut.at( ivar )] = ( dv[ivar] - offset ) * scale * 2 - 1;
  }
}
